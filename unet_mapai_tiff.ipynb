{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net implementation for the MapAI dataset\n",
    "\n",
    "This is my own step-by-step implementation of U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !conda install ipykernel\n",
    "# !ipython kernel install --user --name=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore tensorflow warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define random seed\n",
    "seed = 42\n",
    "np.random.seed = seed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folders:  ['images', 'lidar', 'masks']\n",
      "Validation folders:  ['images', 'lidar', 'masks']\n"
     ]
    }
   ],
   "source": [
    "# 1) Define image dimensions and specify data location\n",
    "\n",
    "img_width = 500\n",
    "img_height = 500\n",
    "img_channels = 4\n",
    "\n",
    "# path to mapai dataset\n",
    "train_path = '/home/shymon/data/mapai/train'\n",
    "val_path = '/home/shymon/data/mapai/validation'\n",
    "\n",
    "# Get the list of the subfolders for train/val\n",
    "train_ids = next(os.walk(train_path))[1] \n",
    "val_ids = next(os.walk(val_path))[1]\n",
    "\n",
    "train_ids, val_ids\n",
    "\n",
    "print('Train folders: ', train_ids)\n",
    "print('Validation folders: ', val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np, pandas as  pd\n",
    "from ipywidgets import interact, interactive, IntSlider, Select, RadioButtons, fixed, BoundedIntText\n",
    "\n",
    "#### Plots ####\n",
    "\n",
    "def plot_image_and_masks_from_df(imgidx, df, figsize=6, with_segm=True):\n",
    "    imgfn = str(df.iloc[imgidx]['image'])\n",
    "    if with_segm:\n",
    "        maskfn = df.iloc[imgidx]['mask']\n",
    "    f, ax = plt.subplots(figsize=(figsize,figsize))\n",
    "    img = Image.open(imgfn)\n",
    "    ax.imshow(img)\n",
    "    if with_segm:\n",
    "        mask = Image.open(maskfn)\n",
    "        ax.imshow(mask, alpha=0.3)\n",
    "    imgid = imgfn.split(\"/\")[-1].split(\".\")[0]\n",
    "    imgsz = img.size\n",
    "    ax.set_title(f'{imgid}, {str(imgsz)}')\n",
    "    ax.set_axis_off()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_image_lidar_and_masks_from_df(imgidx, df, figsize=6, with_segm=True):\n",
    "    imgfn = str(df.iloc[imgidx]['image'])\n",
    "    lidarfn = str(df.iloc[imgidx]['lidar'])\n",
    "    if with_segm:\n",
    "        maskfn = df.iloc[imgidx]['mask']\n",
    "    f, ax = plt.subplots(1,2, figsize=(figsize,figsize))\n",
    "    img = Image.open(imgfn)\n",
    "    lidar = np.array(Image.open(lidarfn))\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(img)\n",
    "    if with_segm:\n",
    "        mask = Image.open(maskfn)\n",
    "        ax[0].imshow(mask, alpha=0.3)\n",
    "        ax[1].imshow(lidar, alpha=0.3)\n",
    "    imgid = imgfn.split(\"/\")[-1].split(\".\")[0]\n",
    "    imgsz = img.size\n",
    "    ax[0].set_title(f'{imgid}, {str(imgsz)}')\n",
    "    ax[0].set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>lidar</th>\n",
       "      <th>mask</th>\n",
       "      <th>is_val</th>\n",
       "      <th>mask_percentage</th>\n",
       "      <th>is_building</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6179_495_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6179_495_4...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6179_495_4...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.155224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6051_690_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6051_690_8...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6051_690_8...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6121_865_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6121_865_5...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6121_865_5...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.017824</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6173_630_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6173_630_2...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6173_630_2...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6147_481_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6147_481_4...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6147_481_4...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.182664</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  ... is_building\n",
       "0  /home/shymon/data/mapai/train/images/6179_495_...  ...        True\n",
       "1  /home/shymon/data/mapai/train/images/6051_690_...  ...       False\n",
       "2  /home/shymon/data/mapai/train/images/6121_865_...  ...        True\n",
       "3  /home/shymon/data/mapai/train/images/6173_630_...  ...       False\n",
       "4  /home/shymon/data/mapai/train/images/6147_481_...  ...        True\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to pandas file with labels\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this to wherever you want to store data\n",
    "DATADIR = Path(\"/home/shymon/data/mapai\") \n",
    "\n",
    "csv_path = \"/home/shymon/Documents/phd/03_CODE/LAB-Net/csv/train_val_original-2023-01-20.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def update_paths(path_str):\n",
    "    return path_str.replace(\"../../data\", str(DATADIR))\n",
    "\n",
    "df['image'] = df['image'].apply(update_paths)\n",
    "df['lidar'] = df['lidar'].apply(update_paths)\n",
    "df['mask'] = df['mask'].apply(update_paths)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7235bcc38b4ae5a91d221772f3f61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='imgidx', max=8499), BoundedIntText(value=4, descripâ€¦"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive plot\n",
    "interactive_plot = interactive(plot_image_and_masks_from_df, df=fixed(df),\n",
    "                            imgidx = BoundedIntText(min=0, max=len(df)-1, step=1, value=0),\n",
    "                               figsize = BoundedIntText(min=4, max=12, step=1, value=4),\n",
    "                               with_segm= RadioButtons(options=[True,False], value=True, \n",
    "                                                      description=\"With segmentation\"))\n",
    "\n",
    "output = interactive_plot.children[-1]\n",
    "\n",
    "# if i does not work, restart kernel\n",
    "interactive_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Resizing training images\n",
    "\n",
    "1. Resizing images is a critical preprocessing step in computer vision. Machine learning models tran faster on smaller images.\n",
    "2. Images require normalization too. We need to divide the image matrix by dividing the pixel values by 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the images and resize them\n",
    "X_train = np.zeros((len(train_ids), img_height, img_width, img_channels), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), img_height, img_width, 1), dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will try without resizing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Start building UNet model\n",
    "# https://github.com/EhabR98/Image_segmentation_Unet-Tutorial\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# from test_utils import summary, comparator # import error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert images to tensors and apply normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images and masks:  8500 8500\n"
     ]
    }
   ],
   "source": [
    "df['image'] #, df['lidar'], df['mask']\n",
    "\n",
    "img_list_ds = tf.data.Dataset.list_files(df['image'], shuffle = False)\n",
    "mask_list_ds = tf.data.Dataset.list_files(df['mask'], shuffle = False)\n",
    "\n",
    "images_fnames = tf.constant(df['image'])\n",
    "masks_fnames = tf.constant(df['mask'])\n",
    "\n",
    "print('# of images and masks: ', len(images_fnames), len(masks_fnames)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/home/shymon/data/mapai/train/images/6179_495_44.tif', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/shymon/data/mapai/train/masks/6179_495_44.tif', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((images_fnames, masks_fnames))\n",
    "\n",
    "for image, mask in dataset.take(1):\n",
    "    print(image)\n",
    "    print(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Preprocess the data\n",
    "\n",
    "Does resizing an image lead to loss of image quality?\n",
    "Yes, resizing images can lead to loss of image quality. This can happen because when you resize an image, you are essentially adding or removing pixels. If you are increasing the size of an image, you need to add pixels, which can cause the image to look blurry or pixelated. If you are decreasing the size of an image, you need to remove pixels, which can cause important details to be lost. The extent of the loss of quality depends on various factors such as the resizing algorithm used, the size of the change in the image dimensions, and the complexity of the image content. To minimize loss of image quality when resizing, you can use resizing algorithms that preserve the edges and details in the image, such as bicubic interpolation or Lanczos resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(480, 480, None), dtype=tf.float32, name=None), TensorSpec(shape=(480, 480, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions to decode images\n",
    "\n",
    "# library needed to decode_tiff\n",
    "# https://www.tensorflow.org/io/api_docs/python/tfio/experimental/image/decode_tiff\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "def process_path(image_path, mask_path):\n",
    "\n",
    "    # image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tfio.experimental.image.decode_tiff(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    # mask\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tfio.experimental.image.decode_tiff(mask)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    return img, mask\n",
    "\n",
    "# Question does U-Net need to have 128x128 input image size? No, images can be any size.\n",
    "# Question do we need to resize images? Images need be resized to a shape that can be divided by 32.\n",
    "# In our case this would be 480 or 512 to upsize, since input is 500 x 500.\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    input_image = tf.image.resize(image, (480, 480), method='bicubic')\n",
    "    input_mask = tf.image.resize(mask, (480, 480), method='bicubic')\n",
    "\n",
    "    # normalization\n",
    "    input_image = input_image / 255.\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "#image_ds = tf.data.Dataset.from_tensor_slices(process_path).map(lambda x: tf.numpy_function(decode_tiff, [x], Tout=tf.uint8))\n",
    "#process_image_ds = image_ds.map(preprocess)\n",
    "\n",
    "image_ds = dataset.map(process_path)\n",
    "process_image_ds = image_ds.map(preprocess)\n",
    "\n",
    "process_image_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: U-Net\n",
    "\n",
    "U-Net, named for its U-shape, was originally created in 2015 for tumor detection, but in the years since has become a very popular choice for other semantic segmentation tasks.\n",
    "\n",
    "U-Net builds on a previous architecture called the Fully Convolutional Network, or FCN, which replaces the dense layers found in a typical CNN with a transposed convolution layer that upsamples the feature map back to the size of the original input image, while preserving the spatial information. This is necessary because the dense layers destroy spatial information (the \"where\" of the image), which is an essential part of image segmentation tasks. An added bonus of using transpose convolutions is that the input size no longer needs to be fixed, as it does when dense layers are used.\n",
    "\n",
    "Unfortunately, the final feature layer of the FCN suffers from information loss due to downsampling too much. It then becomes difficult to upsample after so much information has been lost, causing an output that looks rough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet implementation\n",
    "# https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial118_binary_semantic_segmentation_using_unet.ipynb\n",
    "\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Conv2D, Activation, Concatenate, Conv2DTranspose\n",
    "from keras.layers import MaxPooling2D, UpSampling2D, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPooling2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 480, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out which image size do we need as input\n",
    "# Answer: 480\n",
    "\n",
    "#for i in range(0,512):\n",
    "#    #print(i)\n",
    "#    if i % 32 == 0:\n",
    "#        print(i)\n",
    "\n",
    "# Define input shape\n",
    "\n",
    "img_height, img_width, img_channels = (480, 480, 4)\n",
    "\n",
    "input_shape = (img_height, img_width, img_channels)\n",
    "img_height, img_width, img_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'previous_block_activation' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Compile U-Net\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m unet(input_shape, n_classes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36munet\u001b[0;34m(img_shape, n_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39mimg_shape)\n\u001b[1;32m     14\u001b[0m \u001b[39m# rescale images from (0, 255) to (0, 1)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rescale \u001b[39m=\u001b[39m previous_block_activation\n\u001b[1;32m     17\u001b[0m contraction \u001b[39m=\u001b[39m {}\n\u001b[1;32m     18\u001b[0m \u001b[39m# # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'previous_block_activation' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Compile U-Net\n",
    "model = build_unet(input_shape, n_classes=1)\n",
    "model.compile(optimizer=Adam(learning_rate = 1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 124, 124, 256), (None, 125, 125, 256)]\n",
    "# https://github.com/qubvel/segmentation_models/issues/1\n",
    "# Solution: Height and width of input images should be divisible by 32 for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "# sudo apt install graphviz\n",
    "\n",
    "# Plot model\n",
    "# tf.keras.utils.plot_model(model, \"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Requires tif file format -> solved\n",
    "\n",
    "EPOCHS = 5\n",
    "VAL_SUBSPLITS = 5\n",
    "BUFFER_SIZE = 850 # https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n",
    "BATCH_SIZE = 4\n",
    "process_image_ds.batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset = process_image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(process_image_ds.element_spec)\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('/home/shymon/Documents/phd/03_CODE/models/mapai_5_epochs.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously saved model\n",
    "from keras.models import load_model\n",
    "model = load_model('/home/shymon/Documents/phd/03_CODE/models/mapai_5_epochs.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test dataset --> X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOU - Intersection Over Union\n",
    "yp = model.predict(X_test) # need to read test dataset\n",
    "yp_threshold = yp > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics --> what metrics to use?\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "n_classes = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(yp, Y_test)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold\n",
    "\n",
    "threshold = 0.5\n",
    "test_img_number = random.randint(0, len(X_test)-1)\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_input=np.expand_dims(test_img, 0)\n",
    "print(test_img_input.shape)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "print(prediction.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further work:\n",
    "- add regularization\n",
    "- implement UNet++ and compare results\n",
    "- link: https://www.kaggle.com/code/ekhtiar/tf-tutorial-semantic-segmentation-with-u-net/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f83ca9a9a2cf1c5a50edfef5010e56ecdbf37be12bf6c9bb673c4fe96327737f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
