{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net implementation for the MapAI dataset\n",
    "\n",
    "This is my own step-by-step implementation of U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !conda install ipykernel\n",
    "# !ipython kernel install --user --name=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore tensorflow warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define random seed\n",
    "seed = 42\n",
    "np.random.seed = seed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train folders:  ['images', 'lidar', 'masks']\n",
      "Validation folders:  ['images', 'lidar', 'masks']\n"
     ]
    }
   ],
   "source": [
    "# 1) Define image dimensions and specify data location\n",
    "\n",
    "img_width = 500\n",
    "img_height = 500\n",
    "img_channels = 3\n",
    "\n",
    "# path to mapai dataset\n",
    "train_path = '/home/shymon/data/mapai/train'\n",
    "val_path = '/home/shymon/data/mapai/validation'\n",
    "\n",
    "# Get the list of the subfolders for train/val\n",
    "train_ids = next(os.walk(train_path))[1] \n",
    "val_ids = next(os.walk(val_path))[1]\n",
    "\n",
    "train_ids, val_ids\n",
    "\n",
    "print('Train folders: ', train_ids)\n",
    "print('Validation folders: ', val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np, pandas as  pd\n",
    "from ipywidgets import interact, interactive, IntSlider, Select, RadioButtons, fixed, BoundedIntText\n",
    "\n",
    "#### Plots ####\n",
    "\n",
    "def plot_image_and_masks_from_df(imgidx, df, figsize=6, with_segm=True):\n",
    "    imgfn = str(df.iloc[imgidx]['image'])\n",
    "    if with_segm:\n",
    "        maskfn = df.iloc[imgidx]['mask']\n",
    "    f, ax = plt.subplots(figsize=(figsize,figsize))\n",
    "    img = Image.open(imgfn)\n",
    "    ax.imshow(img)\n",
    "    if with_segm:\n",
    "        mask = Image.open(maskfn)\n",
    "        ax.imshow(mask, alpha=0.3)\n",
    "    imgid = imgfn.split(\"/\")[-1].split(\".\")[0]\n",
    "    imgsz = img.size\n",
    "    ax.set_title(f'{imgid}, {str(imgsz)}')\n",
    "    ax.set_axis_off()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_image_lidar_and_masks_from_df(imgidx, df, figsize=6, with_segm=True):\n",
    "    imgfn = str(df.iloc[imgidx]['image'])\n",
    "    lidarfn = str(df.iloc[imgidx]['lidar'])\n",
    "    if with_segm:\n",
    "        maskfn = df.iloc[imgidx]['mask']\n",
    "    f, ax = plt.subplots(1,2, figsize=(figsize,figsize))\n",
    "    img = Image.open(imgfn)\n",
    "    lidar = np.array(Image.open(lidarfn))\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(img)\n",
    "    if with_segm:\n",
    "        mask = Image.open(maskfn)\n",
    "        ax[0].imshow(mask, alpha=0.3)\n",
    "        ax[1].imshow(lidar, alpha=0.3)\n",
    "    imgid = imgfn.split(\"/\")[-1].split(\".\")[0]\n",
    "    imgsz = img.size\n",
    "    ax[0].set_title(f'{imgid}, {str(imgsz)}')\n",
    "    ax[0].set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>lidar</th>\n",
       "      <th>mask</th>\n",
       "      <th>is_val</th>\n",
       "      <th>mask_percentage</th>\n",
       "      <th>is_building</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6179_495_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6179_495_4...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6179_495_4...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.155224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6051_690_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6051_690_8...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6051_690_8...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6121_865_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6121_865_5...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6121_865_5...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.017824</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6173_630_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6173_630_2...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6173_630_2...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/shymon/data/mapai/train/images/6147_481_...</td>\n",
       "      <td>/home/shymon/data/mapai/train/lidar/6147_481_4...</td>\n",
       "      <td>/home/shymon/data/mapai/train/masks/6147_481_4...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.182664</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  \\\n",
       "0  /home/shymon/data/mapai/train/images/6179_495_...   \n",
       "1  /home/shymon/data/mapai/train/images/6051_690_...   \n",
       "2  /home/shymon/data/mapai/train/images/6121_865_...   \n",
       "3  /home/shymon/data/mapai/train/images/6173_630_...   \n",
       "4  /home/shymon/data/mapai/train/images/6147_481_...   \n",
       "\n",
       "                                               lidar  \\\n",
       "0  /home/shymon/data/mapai/train/lidar/6179_495_4...   \n",
       "1  /home/shymon/data/mapai/train/lidar/6051_690_8...   \n",
       "2  /home/shymon/data/mapai/train/lidar/6121_865_5...   \n",
       "3  /home/shymon/data/mapai/train/lidar/6173_630_2...   \n",
       "4  /home/shymon/data/mapai/train/lidar/6147_481_4...   \n",
       "\n",
       "                                                mask  is_val  mask_percentage  \\\n",
       "0  /home/shymon/data/mapai/train/masks/6179_495_4...   False         0.155224   \n",
       "1  /home/shymon/data/mapai/train/masks/6051_690_8...   False         0.000000   \n",
       "2  /home/shymon/data/mapai/train/masks/6121_865_5...   False         0.017824   \n",
       "3  /home/shymon/data/mapai/train/masks/6173_630_2...   False         0.000000   \n",
       "4  /home/shymon/data/mapai/train/masks/6147_481_4...   False         0.182664   \n",
       "\n",
       "   is_building  \n",
       "0         True  \n",
       "1        False  \n",
       "2         True  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to pandas file with labels\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this to wherever you want to store data\n",
    "DATADIR = Path(\"/home/shymon/data/mapai\") \n",
    "\n",
    "csv_path = \"/home/shymon/Documents/phd/03_CODE/LAB-Net/csv/train_val_original-2023-01-20.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def update_paths(path_str):\n",
    "    return path_str.replace(\"../../data\", str(DATADIR))\n",
    "\n",
    "df['image'] = df['image'].apply(update_paths)\n",
    "df['lidar'] = df['lidar'].apply(update_paths)\n",
    "df['mask'] = df['mask'].apply(update_paths)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fb31b15fea48d69e3fb056f892603e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='imgidx', max=8499), BoundedIntText(value=4, descripâ€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive plot\n",
    "interactive_plot = interactive(plot_image_and_masks_from_df, df=fixed(df),\n",
    "                            imgidx = BoundedIntText(min=0, max=len(df)-1, step=1, value=0),\n",
    "                               figsize = BoundedIntText(min=4, max=12, step=1, value=4),\n",
    "                               with_segm= RadioButtons(options=[True,False], value=True, \n",
    "                                                      description=\"With segmentation\"))\n",
    "\n",
    "output = interactive_plot.children[-1]\n",
    "\n",
    "# if i does not work, restart kernel\n",
    "interactive_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Resizing training images\n",
    "\n",
    "1. Resizing images is a critical preprocessing step in computer vision. Machine learning models tran faster on smaller images.\n",
    "2. Images require normalization too. We need to divide the image matrix by dividing the pixel values by 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the images and resize them\n",
    "X_train = np.zeros((len(train_ids), img_height, img_width, img_channels), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), img_height, img_width, 1), dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will try without resizing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Start building UNet model\n",
    "# https://github.com/EhabR98/Image_segmentation_Unet-Tutorial\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# from test_utils import summary, comparator # import error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert images to tensors and apply normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images and masks:  8500 8500\n"
     ]
    }
   ],
   "source": [
    "df['image'] #, df['lidar'], df['mask']\n",
    "\n",
    "img_list_ds = tf.data.Dataset.list_files(df['image'], shuffle = False)\n",
    "mask_list_ds = tf.data.Dataset.list_files(df['mask'], shuffle = False)\n",
    "\n",
    "images_fnames = tf.constant(df['image'])\n",
    "masks_fnames = tf.constant(df['mask'])\n",
    "\n",
    "print('# of images and masks: ', len(images_fnames), len(masks_fnames)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/home/shymon/data/mapai/train/images/6179_495_44.tif', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/shymon/data/mapai/train/masks/6179_495_44.tif', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((images_fnames, masks_fnames))\n",
    "\n",
    "for image, mask in dataset.take(1):\n",
    "    print(image)\n",
    "    print(mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Preprocess the data\n",
    "\n",
    "Does resizing an image lead to loss of image quality?\n",
    "Yes, resizing images can lead to loss of image quality. This can happen because when you resize an image, you are essentially adding or removing pixels. If you are increasing the size of an image, you need to add pixels, which can cause the image to look blurry or pixelated. If you are decreasing the size of an image, you need to remove pixels, which can cause important details to be lost. The extent of the loss of quality depends on various factors such as the resizing algorithm used, the size of the change in the image dimensions, and the complexity of the image content. To minimize loss of image quality when resizing, you can use resizing algorithms that preserve the edges and details in the image, such as bicubic interpolation or Lanczos resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(480, 480, 3), dtype=tf.float32, name=None), TensorSpec(shape=(480, 480, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions to decode images\n",
    "\n",
    "def process_path(image_path, mask_path):\n",
    "\n",
    "    # image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3) # for png -> works\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    # mask\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    return img, mask\n",
    "\n",
    "# Question does U-Net need to have 128x128 input image size? No, images can be any size.\n",
    "# Question do we need to resize images? Images need be resized to a shape that can be divided by 32.\n",
    "# In our case this would be 480 or 512 to upsize, since input is 500 x 500.\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    input_image = tf.image.resize(image, (480, 480), method='bicubic')\n",
    "    input_mask = tf.image.resize(mask, (480, 480), method='bicubic')\n",
    "\n",
    "    # normalization\n",
    "    input_image = input_image / 255.\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "image_ds = dataset.map(process_path)\n",
    "process_image_ds = image_ds.map(preprocess)\n",
    "\n",
    "process_image_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: U-Net\n",
    "\n",
    "U-Net, named for its U-shape, was originally created in 2015 for tumor detection, but in the years since has become a very popular choice for other semantic segmentation tasks.\n",
    "\n",
    "U-Net builds on a previous architecture called the Fully Convolutional Network, or FCN, which replaces the dense layers found in a typical CNN with a transposed convolution layer that upsamples the feature map back to the size of the original input image, while preserving the spatial information. This is necessary because the dense layers destroy spatial information (the \"where\" of the image), which is an essential part of image segmentation tasks. An added bonus of using transpose convolutions is that the input size no longer needs to be fixed, as it does when dense layers are used.\n",
    "\n",
    "Unfortunately, the final feature layer of the FCN suffers from information loss due to downsampling too much. It then becomes difficult to upsample after so much information has been lost, causing an output that looks rough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet implementation\n",
    "# https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial118_binary_semantic_segmentation_using_unet.ipynb\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "\n",
    "img_height, img_width, img_channels = (480, 480, 3)\n",
    "\n",
    "input_shape = (img_height, img_width, img_channels)\n",
    "img_height, img_width, img_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which image size do we need as input\n",
    "# Answer: 480\n",
    "\n",
    "for i in range(0,512):\n",
    "    #print(i)\n",
    "    if i % 32 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile U-Net\n",
    "model = build_unet(input_shape, n_classes=1)\n",
    "model.compile(optimizer=Adam(learning_rate = 1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 124, 124, 256), (None, 125, 125, 256)]\n",
    "# https://github.com/qubvel/segmentation_models/issues/1\n",
    "# Solution: Height and width of input images should be divisible by 32 for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "# sudo apt install graphviz\n",
    "\n",
    "# Plot model\n",
    "tf.keras.utils.plot_model(model, \"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Requires tif file format\n",
    "\n",
    "EPOCHS = 5\n",
    "VAL_SUBSPLITS = 5\n",
    "BUFFER_SIZE = 850 # https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n",
    "BATCH_SIZE = 32\n",
    "process_image_ds.batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset = process_image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(process_image_ds.element_spec)\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f83ca9a9a2cf1c5a50edfef5010e56ecdbf37be12bf6c9bb673c4fe96327737f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
